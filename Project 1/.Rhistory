}
plot(sum_within_differences)
km_opt <- kmodes(cluster_data1, 3, iter.max = 300)
feature <- 'vote'
cluster_allocation <- split(seq(1, 8490), factor(km_opt$cluster))
for(alloc in cluster_allocation){
votes_cluster <- votes_data[unlist(alloc)]
print(table(votes_cluster))
}
clustering_results <- cluster_data1
clustering_results$'Cluster' <- km_opt$cluster
p1 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = gender), position = 'fill')
p2 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = rural), position = 'fill')
p3 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = age_group), position = 'fill')
p4 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = full_time_job), position = 'fill')
#p5 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = education_level), position = 'fill')
#p6 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = children), position = 'fill')
#p7 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = awareness), position = 'fill')
p8 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = vote), position = 'fill')
#p9 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = effect), position = 'fill')
grid.arrange(p1,
p2,
p3,
p4,
#             p5,
#             p6,
#             p7,
p8,
#             p9,
nrow = 3)
features_to_cluster1 <- c( 4, 5, 6, 8, 12)
cluster_data1 <- mod_data[,features_to_cluster1]
features_to_cluster1 <- c( 4, 5, 6, 8, 12)
cluster_data1 <- mod_data[,features_to_cluster1]
features = names(cluster_data1)
max_num_clusters <- 12
sum_within_differences <- rep(0, max_num_clusters)
for(i in 1:max_num_clusters){
km <- kmodes(cluster_data1, i, iter.max = 300)
sum_within_differences[i] = sum(km$withindiff)
}
plot(sum_within_differences)
km_opt <- kmodes(cluster_data1, 3, iter.max = 300)
feature <- 'vote'
cluster_allocation <- split(seq(1, 8490), factor(km_opt$cluster))
clustering_results <- cluster_data1
clustering_results$'Cluster' <- km_opt$cluster
p1 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = gender), position = 'fill')
p2 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = rural), position = 'fill')
p3 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = age_group), position = 'fill')
p4 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = full_time_job), position = 'fill')
#p5 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = education_level), position = 'fill')
#p6 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = children), position = 'fill')
#p7 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = awareness), position = 'fill')
p8 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = vote), position = 'fill')
grid.arrange(p1,
p2,
p3,
p4,
#             p5,
#             p6,
#             p7,
p8,
#             p9,
nrow = 3)
hcl <- hclust(cluster_data1)
features_to_cluster1 <- c( 4, 5, 6, 8, 12)
cluster_data1 <- mod_data[,features_to_cluster1]
features = names(cluster_data1)
max_num_clusters <- 12
sum_within_differences <- rep(0, max_num_clusters)
for(i in 1:max_num_clusters){
km <- kmodes(cluster_data1, i, iter.max = 300)
sum_within_differences[i] = sum(km$withindiff)
}
plot(sum_within_differences)
km_opt <- kmodes(cluster_data1, 3, iter.max = 300)
feature <- 'vote'
cluster_allocation <- split(seq(1, 8490), factor(km_opt$cluster))
for(alloc in cluster_allocation){
votes_cluster <- votes_data[unlist(alloc)]
print(table(votes_cluster))
}
clustering_results <- cluster_data1
clustering_results$'Cluster' <- km_opt$cluster
p1 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = gender), position = 'fill')
p2 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = rural), position = 'fill')
p3 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = age_group), position = 'fill')
p4 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = full_time_job), position = 'fill')
#p5 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = education_level), position = 'fill')
#p6 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = children), position = 'fill')
#p7 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = awareness), position = 'fill')
p8 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = vote), position = 'fill')
#p9 <- ggplot(data = clustering_results, aes(Cluster)) + geom_bar(aes(fill = effect), position = 'fill')
grid.arrange(p1,
p2,
p3,
p4,
#             p5,
#             p6,
#             p7,
p8,
#             p9,
nrow = 3)
grid.arrange(p1,
p2,
p3,
p4,
#             p5,
#             p6,
#             p7,
p8,
#             p9,
nrow = 3)
setwd('C:\Users\Jacob Lindbäck\Documents\GitHub\Statistical-Learning-For-Big-Data_Mini-2')
setwd('C:/Users/Jacob Lindbäck/Documents/GitHub/Statistical-Learning-For-Big-Data_Mini-2')
library(ggplot2)
library(GGally)
library(glmnet)
library(randomForest)
library(rpart)
library(class)
library(caret)
grade_to_binary <- function(x){
if(x > 7){return(as.factor('Greater than 7'))}
return(as.factor('Less or equal to 7'))
}
isRenovated = function(x){
if(abs(x) < 1){return(as.factor('No'))}
return(as.factor('Yes'))
}
hasBasement = function(x){
if(abs(x) < 0.5){return(as.factor('No'))}
return(as.factor('Yes'))
}
house_data <- read.csv('Data/housing.csv')
house_data$'grade_binary' <- sapply(house_data$'grade', grade_to_binary)
house_data$'renovated' <- sapply(house_data$'yr_renovated', isRenovated)
house_data$'has_Basement' <- sapply(house_data$'sqft_basement', hasBasement)
num_obs <- nrow(house_data)
features <- names(house_data)
features_to_remove <- c('id', 'date', 'zipcode')
col_to_remove <- !(features %in% features_to_remove)
num_train <- 10000
num_test <- 10000
set.seed(42)
train_test_data <- house_data[sample(1:num_obs,num_test+num_train, replace = FALSE),]
train_data <- train_test_data[1:num_train,]
test_data <- train_test_data[-(1:num_train),]
train_data <- train_data[,col_to_remove]
test_data <- test_data[,col_to_remove]
features_to_plot <- !(names(train_data) %in% c('yr_renovated', 'sqft_basement'))
ggpairs(train_data[1:500,features_to_plot], mapping=ggplot2::aes(colour = grade_binary))
ggplot(train_data[(train_data$renovated == 'Yes'),], aes(x = yr_renovated, y = grade_binary)) + geom_point(alpha=0.1)
form <- grade ~ price + bedrooms + bathrooms + sqft_living + sqft_lot + floors + waterfront +
view + condition + sqft_above +  yr_built + sqft_living15 + sqft_lot15 + renovated*yr_renovated + has_Basement*sqft_basement
x_train <- model.matrix(form, train_data)
mod1_lasso <- cv.glmnet(x_train, train_data$grade_binary, family = 'multinomial')
x_test <- model.matrix(form, test_data)
predictedOdds <- predict(mod1_lasso, x_test)
predictions1 <- sapply(predictedOdds, function(x){if(x>=0){return('Greater than 7')}; return('Less or equal to 7')})
correct_pred_lasso <- which(predictions1 == test_data$grade_binary)
p1 <- length(correct_pred_lasso)/num_test
mod1_lasso <- cv.glmnet(x_train, train_data$grade, family = 'multinomial')
x_test <- model.matrix(form, test_data)
predictedOdds <- predict(mod1_lasso, x_test)
predictions1 <- sapply(predictedOdds, function(x){if(x>=0){return('Greater than 7')}; return('Less or equal to 7')})
correct_pred_lasso <- which(predictions1 == test_data$grade)
p1 <- length(correct_pred_lasso)/num_test
predictions1
predictedOdds
predictedOdds>0
dim(predictedOdds)
levels(train_data$grade)
table(train_data$grade)
library(klaR)
partimat(form_no_inter, data = train_data)
form_no_inter <- grade_binary ~ price + bedrooms + bathrooms + sqft_living + sqft_lot + floors + waterfront +
view + condition + sqft_above +  yr_built + sqft_living15 + sqft_lot15 + renovated + yr_renovated + has_Basement + sqft_basement
partimat(form_no_inter, data = train_data, method = 'lda')
partimat(form_no_inter, data = train_data, method = 'lda', nplots.vert = 3, nplots.hor = 3)
ldafit <- train(form_no_inter, data = train_data, method = 'lda')
warnings()
ldafit <- train(form_no_inter, data = train_data, method = 'qda')
#########################################
################# DA ####################
#########################################
trControl <-trainControl(method="repeatedcv",repeats=10,summaryFunction=multiClassSummary)
ldafit <- train(form_no_inter, data = train_data, method = 'qda', tunelength = 15, trControl= trControl)
ldafit <- train(form_no_inter, data = train_data, method = 'qda', tunelength = 15, trControl= trControl, metric = 'Accuracy')
trControl <- trainControl(method = 'cv', number = 10)
knn_fit <- train(form_no_inter, method = 'knn', trControl = trControl, metric = "Accuracy", data = train_data)
predictions4 <- predict(knn_fit, test_data)
correct_pred_knn <- which(predictions4 == test_data$grade_binary)
p4 <- length(correct_pred_knn)/num_test
plot(knn_fit)
#########################################
################# DA ####################
#########################################
trControl <-trainControl(method="cv",repeats=10,summaryFunction=multiClassSummary)
#########################################
################# DA ####################
#########################################
trControl <-trainControl(method="cv", numer=10,summaryFunction=multiClassSummary)
#########################################
################# DA ####################
#########################################
trControl <-trainControl(method="cv", number=10,summaryFunction=multiClassSummary)
ldafit <- train(form_no_inter, data = train_data, method = 'qda', tunelength = 15, trControl= trControl, metric = 'Accuracy')
ldafit <- train(form_no_inter, data = train_data, method = 'qda', trControl= trControl, metric = 'Accuracy')
warnings()
summary(train_data[(train_data$grade_binary == 'Less or equal to 7'),])
summary(train_data[(train_data$grade_binary != 'Less or equal to 7'),])
setwd('C:/Users/Jacob Lindbäck/Documents/GitHub/tidsserier/Project 1')
#Load required packages
library('R.matlab')
library('gridExtra')
library('Matrix')
library('ggplot2')
my_mean <- function(x) {return(sum(x)/length(x))}
my_acf <- function(x, max.lag = 1){
xbar <- my_mean(x)
num_obs <- length(x)
x_shift <- x-xbar
gamma <- rep(0,max.lag+1)
for(i in 0:max.lag){
gamma[i+1] <- (1/num_obs) * sum(x_shift[(i+1):num_obs]*x_shift[1:(num_obs-i)])
}
return(gamma)
}
my_acf_matrix <- function(gamma){
max.lag <- length(gamma) - 1
Gamma = matrix(rep(0,(max.lag+1)^2), max.lag + 1, max.lag+1)
for(i in 1:(max.lag+1)){
for(j in i:(max.lag+1)) {
Gamma[i,j] = gamma[(j-i)+1]
Gamma[j,i] = gamma[(j-i)+1]
}
}
return(Gamma)
}
###################################################################
data <- readMat('exchangerate.mat')
data <- data$data
intr_value = data
abs_returns = data[2:205,] - data[1:204]
log_returns = log(data[2:205,]) - log(data[1:204])
abs_returns = abs_returns - mean(abs_returns)
log_returns = log_returns - mean(log_returns)
intr_value = intr_value - mean(intr_value)
exchange_data = data.frame(time = seq(2,205), absolute.return = abs_returns, log.returns = log_returns, intrinsic.value = intr_value[2:205])
p1 <- ggplot(data = exchange_data)+ geom_line(aes(x = time, y = abs_returns))
p1
p1 <- ggplot(data = exchange_data, aes(x = time, y = abs_returns))+ geom_line() + geom_smooth()
p1
p2 <- ggplot(data = exchange_data, aes(x = time, y = log_returns))+ geom_line() + geom_smooth(())
p2 <- ggplot(data = exchange_data, aes(x = time, y = log_returns))+ geom_line() + geom_smooth(
d7
)
p2 <- ggplot(data = exchange_data, aes(x = time, y = log_returns))+ geom_line() + geom_smooth()
p2
p1 <- ggplot(data = exchange_data, aes(x = time, y = abs_returns))+ geom_line() + geom_smooth(method = 'loess')
p1
p2 <- ggplot(data = exchange_data, aes(x = time, y = log_returns))+ geom_line() + geom_smooth(method = 'loess')
p2
p1 <- ggplot(data = exchange_data, aes(x = time, y = abs_returns))+ geom_line() + geom_smooth(method = 'loess')
p1
p2 <- ggplot(data = exchange_data, aes(x = time, y = log_returns))+ geom_line() + geom_smooth(method = 'loess')
p2
p3 <- ggplot(data = exchange_data, aes(x = time, y = intrinsic.value))+ geom_line() + geom_smooth(method = 'loess')
p3
p1 <- ggplot(data = exchange_data, aes(x = time, y = abs_returns))+ geom_line() + geom_smooth(method = 'loess') + theme_bw()
p1
p2 <- ggplot(data = exchange_data, aes(x = time, y = log_returns))+ geom_line() + geom_smooth(method = 'loess') + theme_bw()
p2
p3 <- ggplot(data = exchange_data, aes(x = time, y = intrinsic.value))+ geom_line() + geom_smooth(method = 'loess') + theme_bw()
p3
p3 <- ggplot(data = exchange_data, aes(x = time, y = intrinsic.value))+ geom_line() + geom_smooth(method = 'loess') + geom_smooth(method = 'lm) + theme_bw()
p3
acf(log_returns)
#########################
##### Problem 2 #########
#########################
#OsÃ¤ker pÃ¥ hur mycket vi ska skriva sjÃ¤lva och hur mycket vi fÃ¥r anvÃ¤nda rakt av.
sampMean<-function(data){
output<-sum(data)/length(data)
}
sampVar<-function(data){
n<-length(data)
output<-(1/(n-1))*sum((data-sampMean(data))^2)
}
sampAutoCov<-function(data,lag){
n<-length(data)
mu<-sampMean(data)
stDv<-sqrt(sampVar(data))
lag<-abs(lag)
output<-0
for(t in 1:(n-lag) ){
output<-output+(data[t+lag]-mu)*(data[t]-mu)
}
output<-output/n
}
sampAutoCorr<-function(data,lag){
output<-sampAutoCov(data,lag)/sampAutoCov(data,0)
}
ljungBox<-function(data,lagMax,alpha){
n<-length(data)
testStat<-0
lagMax<-abs(lagMax) #unsure of this, but needed for summation.
for(j in 1:lagMax){
testStat<-testStat+sampAutoCorr(data,j)^2/(n-j)
}
testStat<-testStat*n*(n+2)
pval<-pchisq(testStat,df=lagMax,lower.tail = FALSE)
res<-testStat>qchisq(1-alpha,df=lagMax)
output<-data.frame(testStat,pval,res)
print(output)
}
acfPlotter<-function(data,lagMax,shouldPlot=TRUE,plotName=""){
lag<-seq(0,lagMax)
subFunc<-function(lag){
output<-sampAutoCorr(data,lag)
}
acf<-sapply(lag,subFunc)
if(shouldPlot){
toPlotDf<-data.frame(lag,acf)
q<-ggplot(data=toPlotDf,aes(x=lag,y=acf))+geom_hline(aes(yintercept=0))+geom_segment(aes(xend=lag,yend=0))+ggtitle(plotName)
q
}
}
#### Now do actual task.
alpha=0.05
lag_max=20
acfPlotter(intr_value,lag_max)
acfPlotter(abs_returns,lag_max)
acfPlotter(log_returns,lag_max)
ljungBox(intr_value,lagMax,alpha) #vad Ã¤r kutym att man sÃ¤ger att p-vÃ¤rdet Ã¤r mindre Ã¤n?
ljungBox(abs_returns,lagMax,alpha)
ljungBox(log_returns,lagMax,alpha)
#########################
##### Problem 3 #########
#########################
#Partions the series into a training data set, and a test set.
num_samples = nrow(exchange_data)
num_train_samples = 120
num_test_samples = num_samples - num_train_samples
train_data = exchange_data[1:num_train_samples,]
test_data = exchange_data[(num_train_samples+1):num_samples,]
#Extracts the last [num_predictors] train_data points and store them in a row matrix
num_predictors = 20
predictors = (train_data$log.returns)[(num_train_samples-num_predictors+1):num_train_samples]
predictors = t(matrix(predictors))
#Computes the acf, and imputes the values corresponding to lag greater or equal to [num_train_samples] by zero.
train_acf = my_acf(train_data$log.returns, max.lag = num_train_samples-1)
train_acf = c(train_acf, rep(0, 30))
Gamma = my_acf_matrix(train_acf[1:num_predictors])
coefficients <- matrix(rep(0,num_predictors*num_test_samples), num_predictors, num_test_samples) #Allocate memory for
#coefficents
LU = lu(Gamma)  #Since matrix of the linear system that is to be solved in each iteration is fix. It's appropriate
#LU-decompose the matrix in order to save computations.
LU = expand(LU)
L = LU$L
U = LU$U
P = LU$P
for(h in 1:num_test_samples){
ii = seq(h+1, h+num_predictors)
b = train_acf[ii]
b1 = solve(P,b)
b2 = forwardsolve(L,b1)
b3 = backsolve(U, b2)
coefficients[,h] = as.matrix(b3)
}
predictions = predictors%*%coefficients
df1 = data.frame(time = rep(seq(num_train_samples+1,num_samples),2), log.returns = c(test_data$log.returns, predictions[1,]), type = c(rep('True', num_test_samples), rep('Predictions', num_test_samples)))
#Plots the predicted log-return against time, superimposed with the predicted time series.
p1 <- ggplot(data = df1, aes(x = time, y = log.returns, group = type)) + geom_line(aes(color = type))
p1 <- p1 + xlab('Time') + ylab('Log-returns') + theme(legend.title=element_blank())
#Distribution of the residuals of the naive estimates, and the predicted time series.
df2 = data.frame(Error1 = test_data$log.returns, Error2 = (test_data$log.returns)-predictions[1,])
p2 <- ggplot(data = df2) + geom_histogram(aes(Error1), bins = 15, fill = 'green', alpha = 0.5)
p2 <- p2 + geom_histogram(aes(Error2), bins = 15, fill = 'blue', alpha = 0.5) + xlab('Errors') +ylab('')
grid.arrange(p1, p2, nrow = 2)
MSE1 = mean((test_data$log.returns)^2) #MSE of the "naive estimate"
MSE2 = mean((test_data$log.returns-predictions[1,])^2) #MSE of the predictive model
######################################################################
############ Computes the predictions based on previous predictions###
######################################################################
upd_predictors1 <- predictors[1,]
coeff1 <- coefficients[(coefficients[,1] != 0),1]
new_predictions <- rep(0,num_test_samples)
for(i in 1:num_test_samples){
ith_prediction <- sum(coeff1*upd_predictors1)
new_predictions[i] = ith_prediction
upd_predictors1 <- c(upd_predictors1[2:num_predictors], ith_prediction)
}
df3 = data.frame(time = rep(seq(num_train_samples+1,num_samples),2), log.returns = c(test_data$log.returns, new_predictions), type = c(rep('True', num_test_samples), rep('Predictions', num_test_samples)))
#Plots the predicted log-return against time, superimposed with the predicted time series.
p3 <- ggplot(data = df3, aes(x = time, y = log.returns, group = type)) + geom_line(aes(color = type))
p3 <- p3 + xlab('Time') + ylab('Log-returns') + theme(legend.title=element_blank())
df4 = data.frame(Error1 = test_data$log.returns, Error2 = (test_data$log.returns)-new_predictions)
p4 <- ggplot(data = df4) + geom_histogram(aes(Error1), bins = 15, fill = 'green', alpha = 0.5)
p4 <- p4 + geom_histogram(aes(Error2), bins = 15, fill = 'blue', alpha = 0.5) + xlab('Errors') +ylab('')
grid.arrange(p3, p4, nrow = 2)
MSE3 = mean((test_data$log.returns-new_predictions)^2)
######################################################################
###### Computes the predictions based on previous predictions#########
######################################################################
upd_predictors2 <- predictors[1,]
coeff1 <- coefficients[which(coefficients[,1] != 0),1]
new_predictions2 <- rep(0,num_test_samples)
for(i in 1:num_test_samples){
ith_prediction <- sum(coeff1*upd_predictors2)
new_predictions2[i] = ith_prediction
upd_predictors2 <- c(upd_predictors2[2:num_predictors], test_data$log.returns[i])
}
df5 = data.frame(time = rep(seq(num_train_samples+1,num_samples),2), log.returns = c(test_data$log.returns, new_predictions2), type = c(rep('True', num_test_samples), rep('Predictions', num_test_samples)))
#Plots the predicted log-return against time, superimposed with the predicted time series.
p5 <- ggplot(data = df5, aes(x = time, y = log.returns, group = type)) + geom_line(aes(color = type))
p5 <- p5 + xlab('Time') + ylab('Log-returns') + theme(legend.title=element_blank())
df6 = data.frame(Error1 = test_data$log.returns, Error2 = (test_data$log.returns)-new_predictions2)
p6 <- ggplot(data = df6) + geom_histogram(aes(Error1), bins = 15, fill = 'green', alpha = 0.5)
p6 <- p6 + geom_histogram(aes(Error2), bins = 15, fill = 'blue', alpha = 0.5) + xlab('Errors') +ylab('')
grid.arrange(p5, p6, nrow = 2)
MSE4 = mean((test_data$log.returns-new_predictions)^2)
######################################################################
###### Predict the value of the upcoming day by using the innovation #
###############################algorithm##############################
######################################################################
######################################################################
###### Code chunk that finds "the optimal" number of predictors#######
######################################################################
max_num_predictors <- 50
MSE_list = rep(0,max_num_predictors)
for(num_predictors in 1:max_num_predictors){
predictors = (train_data$log.returns)[(num_train_samples-num_predictors+1):num_train_samples]
predictors = t(matrix(predictors))
Gamma = my_acf_matrix(train_acf[1:num_predictors])
coefficients <- matrix(rep(0,num_predictors*num_test_samples), num_predictors, num_test_samples)
LU = lu(Gamma)
LU = expand(LU)
L = LU$L
U = LU$U
P = LU$P
for(h in 1:num_test_samples){
ii = seq(h+1, h+num_predictors)
b = train_acf[ii]
b1 = solve(P,b)
b2 = forwardsolve(L,b1)
b3 = backsolve(U, b2)
coefficients[,h] = as.matrix(b3)
}
predictions = predictors%*%coefficients
MSE_list[num_predictors] = mean((test_data$log.returns-predictions[1,])^2)
}
df5 = data.frame(MSE = MSE_list, num.predictors = 1:50)
p5 <- ggplot(data = df5, aes(x = num.predictors, y = MSE)) + geom_point() + geom_hline(aes(yintercept = MSE1))
p5 <- p5 + xlab('Number of predictors')
p5
#########################
##### Problem 4 #########
#########################
qqnorm(exchange_data$log.returns)
qqline(exchange_data$log.returns)
X = rnorm(204)
#To do: abs(log.return, Box Ljung)
ljungBox(abs(log_returns),lagMax,alpha)
acfPlotter(abs(log_returns),lagMax)
p3 <- ggplot(data = exchange_data, aes(x = time, y = intrinsic.value))+ geom_line() + geom_smooth(method = 'loess') + geom_smooth(method = 'lm') + theme_bw()
p3
p1 <- ggplot(data = exchange_data, aes(x = time, y = abs_returns))+ geom_line() + geom_smooth(method = 'loess', se = FALSE) + theme_bw()
p1
p2 <- ggplot(data = exchange_data, aes(x = time, y = log_returns))+ geom_line() + geom_smooth(method = 'loess',se = FALSE) + theme_bw()
p2
p3 <- ggplot(data = exchange_data, aes(x = time, y = intrinsic.value))+ geom_line() + geom_smooth(method = 'loess', se = FALSE) + geom_smooth(method = 'lm') + theme_bw()
p3
p3 <- ggplot(data = exchange_data, aes(x = time, y = intrinsic.value))+ geom_line() + geom_smooth(method = 'loess', se = FALSE) + geom_smooth(method = 'lm', se = FALSE, color = 'red') + theme_bw()
p3
p3 <- ggplot(data = exchange_data, aes(x = time, y = intrinsic.value))+ geom_line() + geom_smooth(method = 'loess', se = FALSE)
p3 <- p3 + geom_smooth(method = 'lm', se = FALSE, color = 'red')+ theme_bw() + xlab('Time')
p3
p1 <- ggplot(data = exchange_data, aes(x = time, y = abs_returns))+ geom_line() + geom_smooth(method = 'loess', se = FALSE) + theme_bw() + xlab('Time') + ylab('Absolute Returns')
p1
p2 <- ggplot(data = exchange_data, aes(x = time, y = log_returns))+ geom_line() + geom_smooth(method = 'loess',se = FALSE) + theme_bw() + xlab('Time') + ylab('Log-Returns')
p2
p3 <- ggplot(data = exchange_data, aes(x = time, y = intrinsic.value))+ geom_line() + geom_smooth(method = 'loess', se = FALSE)
p3 <- p3 + geom_smooth(method = 'lm', se = FALSE, color = 'red')+ theme_bw() + xlab('Time') + ylab('Trade-Weighted Index')
p3 <- ggplot(data = exchange_data, aes(x = time, y = intrinsic.value))+ geom_line() + geom_smooth(method = 'loess', se = FALSE)
p3 <- p3 + geom_smooth(method = 'lm', se = FALSE, color = 'red')+ theme_bw() + xlab('Time') + ylab('Trade-Weighted Index')
p3
p1 <- p1 + geom_smooth(method = 'loess', se = FALSE)
p1 <- ggplot(data = exchange_data, aes(x = time, y = abs_returns))+ geom_line() + geom_smooth(method = 'loess', se = FALSE) + theme_bw() + xlab('Time') + ylab('Absolute Returns')
p1
p1 <- p1 + geom_smooth(method = 'loess', se = FALSE)
p1 <- p1 + geom_smooth(method = 'lm', se = FALSE)
p1
p1 <- p1 + geom_smooth(method = 'lm', se = FALSE, color = 'Red')
p1
p2 <- p2 + geom_smooth(method = 'lm', se = FALSE, color = 'Red')
p2
p1 <- ggplot(data = exchange_data, aes(x = time, y = abs_returns))+ geom_line() + geom_smooth(method = 'loess', se = FALSE) + theme_bw() + xlab('Time') + ylab('Absolute Returns') + theme(text = element_text(size=20))
p1 <- p1 + geom_smooth(method = 'lm', se = FALSE, color = 'Red')
p1
p3 <- p3 + geom_smooth(method = 'lm', se = FALSE, color = 'red')+ theme_bw() + xlab('Time') + ylab('Trade-Weighted Index') + theme(text = element_text(size=20))
p1 <- ggplot(data = exchange_data, aes(x = time, y = abs_returns))+ geom_line() + geom_smooth(method = 'loess', se = FALSE) + theme_bw() + xlab('Time') + ylab('Absolute Returns')
p1 <- p1 + geom_smooth(method = 'lm', se = FALSE, color = 'Red') + theme(text = element_text(size=20))
p1
p2 <- ggplot(data = exchange_data, aes(x = time, y = log_returns))+ geom_line() + geom_smooth(method = 'loess',se = FALSE) + theme_bw() + xlab('Time') + ylab('Log-Returns')
p2 <- p2 + geom_smooth(method = 'lm', se = FALSE, color = 'Red') + theme(text = element_text(size=20))
p2
p3 <- ggplot(data = exchange_data, aes(x = time, y = intrinsic.value))+ geom_line() + geom_smooth(method = 'loess', se = FALSE)
p3 <- p3 + geom_smooth(method = 'lm', se = FALSE, color = 'red')+ theme_bw() + xlab('Time') + ylab('Trade-Weighted Index') + theme(text = element_text(size=20))
p3
